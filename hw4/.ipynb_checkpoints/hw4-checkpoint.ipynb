{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 2 Setting up the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as pt\n",
    "import mltools as ml\n",
    "import random\n",
    "\n",
    "random.seed(0)\n",
    "X = np.genfromtxt(\"data/X_train.txt\")\n",
    "Y = np.genfromtxt(\"data/Y_train.txt\")\n",
    "\n",
    "X, Y = ml.shuffleData(X, Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 2.1 . Print the minimum, maximum, mean, and the variance of all of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum of the featurs: \n",
      "[193.0, 190.0, 214.97, 205.42, 10.0, 0.0, 0.0, 0.0, 0.68146, 0.0, 0.0, 0.0, 1.0074, -999.9]\n",
      "\n",
      "Maximum of the featurs: \n",
      "[253.0, 250.5, 252.5, 252.5, 17130.0, 12338.0, 9238.0, 35.796, 19.899, 11.368, 21.466, 14.745, 278.71, 782.5]\n",
      "\n",
      "Mean of the featurs: \n",
      "[241.79722040000001, 228.22826005, 241.79629754999996, 233.64929865000005, 2867.97959, 884.073295, 173.553355, 3.04719571745, 6.351967218050001, 1.9252323192099996, 4.2937934886999995, 2.809471779, 10.3679146455, 7.8733445]\n",
      "\n",
      "Variance of the featurs: \n",
      "[82.69456190782381, 90.95739454607398, 35.725579594364, 95.26085391860819, 10619418.044443432, 3257029.845612843, 740656.133623244, 7.422442772290732, 6.332299131939853, 4.284487034670787, 4.046840868867378, 1.9821830277466972, 166.67925177399363, 1410.79679273432]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "minimum = [\n",
    "    min(X[:, feature])\n",
    "    for feature in range(X.shape[1])\n",
    "]\n",
    "\n",
    "maximum = [\n",
    "    max(X[:, feature])\n",
    "    for feature in range(X.shape[1])\n",
    "]\n",
    "\n",
    "mean = [\n",
    "    np.mean(X[:, feature])\n",
    "    for feature in range(X.shape[1])\n",
    "]\n",
    "\n",
    "variance = [\n",
    "    np.var(X[:, feature])\n",
    "    for feature in range(X.shape[1])\n",
    "]\n",
    "\n",
    "print(\"Minimum of the featurs: \\n{}\\n\".format(minimum))\n",
    "print(\"Maximum of the featurs: \\n{}\\n\".format(maximum))\n",
    "print(\"Mean of the featurs: \\n{}\\n\".format(mean))\n",
    "print(\"Variance of the featurs: \\n{}\\n\".format(variance))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 2.2 Split the dataset, and rescale each into training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum of the featurs: \n",
      "[-5.185159547516007, -3.9904901504099946, -4.383077615382361, -2.8800938487379297, -0.8885717521554883, -0.501137750513952, -0.20518388975750523, -1.0872324341522555, -2.035086132181643, -0.941386078183028, -2.1595615021348302, -2.0032206249240385, -0.6983760287488431, -26.985647443483327]\n",
      "\n",
      "Maximum of the featurs: \n",
      "[1.2240199252850148, 2.0299598274048996, 1.6776289839388163, 1.883476637120893, 4.332221147762579, 6.370000317561288, 10.85665876316197, 11.749402243527156, 4.207888262499242, 4.55856551188295, 6.5737090171537265, 5.402656813919008, 20.15811003837007, 16.613276843340614]\n",
      "\n",
      "Mean of the featurs: \n",
      "[-2.538342869229382e-14, -1.021049911287264e-15, 1.4661338809673908e-14, -3.6828851079917515e-14, 8.526512829121201e-18, -9.947598300641403e-18, -3.836930773104541e-17, 3.009148485944024e-16, 1.5518253349000589e-15, -7.261746759468224e-16, -2.722799763432704e-15, 5.518074885912938e-15, -3.3608671401452737e-15, 4.504840944719035e-16]\n",
      "\n",
      "Variance of the featurs: \n",
      "[1.0000000000000175, 0.9999999999999727, 0.9999999999999966, 1.0000000000000027, 1.0000000000000016, 1.0000000000000089, 0.9999999999999838, 0.9999999999999989, 0.9999999999999997, 0.9999999999999855, 0.999999999999998, 1.0000000000000027, 1.0000000000000135, 1.000000000000061]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Xtr, Xva, Ytr, Yva = ml.splitData(X, Y)\n",
    "Xt, Yt = Xtr[:5000], Ytr[:5000]  # subsample for efficiency (you can go higher)\n",
    "XtS, params = ml.rescale(Xt)  # Normalize the features\n",
    "XvS, _ = ml.rescale(Xva, params)  # Normalize the features\n",
    "\n",
    "minimum = [\n",
    "    min(XtS[:, feature])\n",
    "    for feature in range(XtS.shape[1])\n",
    "]\n",
    "\n",
    "maximum = [\n",
    "    max(XtS[:, feature])\n",
    "    for feature in range(XtS.shape[1])\n",
    "]\n",
    "\n",
    "mean = [\n",
    "    np.mean(XtS[:, feature])\n",
    "    for feature in range(XtS.shape[1])\n",
    "]\n",
    "\n",
    "variance = [\n",
    "    np.var(XtS[:, feature])\n",
    "    for feature in range(XtS.shape[1])\n",
    "]\n",
    "\n",
    "print(\"Minimum of the featurs: \\n{}\\n\".format(minimum))\n",
    "print(\"Maximum of the featurs: \\n{}\\n\".format(maximum))\n",
    "print(\"Mean of the featurs: \\n{}\\n\".format(mean))\n",
    "print(\"Variance of the featurs: \\n{}\\n\".format(variance))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 3. Linear Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6482208865042813"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner = ml.linearC.linearClassify()\n",
    "learner.train(XtS, Yt, reg=0.0, initStep=0.5, stopTol=1e-6, stopIter=100)\n",
    "learner.auc(XtS, Yt)  # train AUC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 3.1 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reg = np.arange(0.0, 10.0, 0.5)\n",
    "auc_val, auc_tr = [], []\n",
    "\n",
    "for r in reg:\n",
    "    learner.train(XtS, Yt, reg=r, initStep=0.5, stopTol=1e-6, stopIter=100)\n",
    "    auc_tr.append(learner.auc(XtS, Yt))\n",
    "    auc_val.append(learner.auc(XvS, Yva))\n",
    "\n",
    "pt.plot(reg, auc_tr, \"r\", label=\"Training UAC\")\n",
    "pt.plot(reg, auc_val, \"b\", label=\"Validation AUC\")\n",
    "pt.xlabel(\"Regularization\")\n",
    "pt.ylabel(\"AUC\")\n",
    "pt.legend()\n",
    "pt.title(\"Plot of AUC vs Regularization parameter\")\n",
    "pt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 3.2 Adding a 2nd degree polynomial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Total number of features before = 14.\n",
    "\n",
    " The new features will consist of all these $14$ old features.\n",
    "\n",
    " Additionally, we will now have all combinations of these features in our second degree polynomial as well.\n",
    "\n",
    " Hence, $+^{14}C_2=91$ features.\n",
    "\n",
    " Finally, we also have $+14$ new features where each feature is a square of the corresponding old feature.\n",
    "\n",
    " Total new features = $14+91+14$\n",
    " $=119 features$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 3.3 AUC performance with second degree polynomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XtP = ml.transforms.fpoly(Xt, 2, bias=False)\n",
    "XvP = ml.transforms.fpoly(Xva, 2, bias=False)\n",
    "XtPS, params = ml.rescale(XtP)\n",
    "XvPS, _ = ml.rescale(XvP, params)\n",
    "\n",
    "auc_val, auc_tr = [], []\n",
    "\n",
    "for r in reg:\n",
    "    learner = ml.linearC.linearClassify()\n",
    "    learner.train(XtPS, Yt, reg=r, initStep=0.5, stopTol=1e-6, stopIter=100)\n",
    "    auc_tr.append(learner.auc(XtPS, Yt))\n",
    "    auc_val.append(learner.auc(XvPS, Yva))\n",
    "\n",
    "pt.plot(reg, auc_tr, \"r\", label=\"Training UAC\")\n",
    "pt.plot(reg, auc_val, \"b\", label=\"Validation AUC\")\n",
    "pt.xlabel(\"Regularization\")\n",
    "pt.ylabel(\"AUC\")\n",
    "pt.legend()\n",
    "pt.title(\"Plot of AUC vs Regularization parameter\")\n",
    "pt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 4 Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = ml.knn.knnClassify()\n",
    "learner.train(XtS, Yt, K=1, alpha=0.0)\n",
    "learner.auc(XtS, Yt)  # train AUC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 4.1 Plot of the training and validation performance for an appropriately wide range of K, with α = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = list(range(1, 10, 2)) + list(range(10, 100, 10))\n",
    "auc_val, auc_tr = [], []\n",
    "\n",
    "for k in K:\n",
    "    learner = ml.knn.knnClassify()\n",
    "    learner.train(XtS, Yt, K=int(k), alpha=0.0)\n",
    "    auc_tr.append(learner.auc(XtS, Yt))\n",
    "    auc_val.append(learner.auc(XvS, Yva))\n",
    "\n",
    "pt.plot(K, auc_tr, \"r\", label=\"Training UAC\")\n",
    "pt.plot(K, auc_val, \"b\", label=\"Validation AUC\")\n",
    "pt.xlabel(\"K\")\n",
    "pt.ylabel(\"AUC\")\n",
    "pt.legend()\n",
    "pt.title(\"Plot of AUC vs the size of the neighborhood(K)\")\n",
    "pt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 4.2 Unscaled/original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = list(range(1, 10, 2)) + list(range(10, 100, 10))\n",
    "auc_val, auc_tr = [], []\n",
    "\n",
    "for k in K:\n",
    "    learner = ml.knn.knnClassify()\n",
    "    learner.train(Xt, Yt, K=k, alpha=0.0)\n",
    "    auc_tr.append(learner.auc(Xt, Yt))\n",
    "    auc_val.append(learner.auc(Xva, Yva))\n",
    "\n",
    "pt.plot(K, auc_tr, \"r\", label=\"Training UAC\")\n",
    "pt.plot(K, auc_val, \"b\", label=\"Validation AUC\")\n",
    "pt.xlabel(\"K\")\n",
    "pt.ylabel(\"AUC\")\n",
    "pt.legend()\n",
    "pt.title(\"Plot of AUC vs the size of the neighborhood(K)\")\n",
    "pt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 4.3 Select both the value of K and α"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = range(1, 50, 5)\n",
    "A = range(0, 5, 1)\n",
    "tr_auc = np.zeros((len(K), len(A)))\n",
    "va_auc = np.zeros((len(K), len(A)))\n",
    "\n",
    "for i, k in enumerate(K):\n",
    "    for j, a in enumerate(A):\n",
    "        learner = ml.knn.knnClassify()\n",
    "        learner.train(Xt, Yt, K=k, alpha=a)\n",
    "        tr_auc[i][j] = learner.auc(Xt, Yt)\n",
    "        va_auc[i][j] = learner.auc(Xva, Yva)\n",
    "\n",
    "f, ax = pt.subplots(1, 1, figsize=(8, 5))\n",
    "cax = ax.matshow(tr_auc, interpolation='nearest')\n",
    "f.colorbar(cax)\n",
    "ax.set_xticklabels(['']+list(A))\n",
    "ax.set_yticklabels(['']+list(K))\n",
    "pt.ylabel(\"K\")\n",
    "pt.xlabel(\"A\")\n",
    "pt.title(\"Training data AUC\")\n",
    "pt.show()\n",
    "\n",
    "f, ax = pt.subplots(1, 1, figsize=(8, 5))\n",
    "cax = ax.matshow(va_auc, interpolation='nearest')\n",
    "f.colorbar(cax)\n",
    "ax.set_xticklabels(['']+list(A))\n",
    "ax.set_yticklabels(['']+list(K))\n",
    "pt.ylabel(\"K\")\n",
    "pt.xlabel(\"A\")\n",
    "pt.title(\"Validation data AUC\")\n",
    "pt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 5. Decision Trees\n",
    " ## 5.1 Vary maxDepth to a range of your choosing, and plot the training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depths = list(range(1, 50, 3))\n",
    "tr_auc, va_auc = [], []\n",
    "\n",
    "for max_depth in max_depths:\n",
    "    learner = ml.dtree.treeClassify(\n",
    "        XtS, Yt, minParent=2, minLeaf=1, maxDepth=max_depth\n",
    "    )\n",
    "    tr_auc.append(learner.auc(XtS, Yt))\n",
    "    va_auc.append(learner.auc(XvS, Yva))\n",
    "\n",
    "pt.plot(max_depths, tr_auc, \"r\", label=\"Training AUC\")\n",
    "pt.plot(max_depths, va_auc, label=\"Validation AUC\")\n",
    "pt.xlabel(\"Max depth\")\n",
    "pt.ylabel(\"AUC\")\n",
    "pt.legend()\n",
    "pt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 5.2 Plot the number of nodes in the tree as maxDepth is varied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1, l2 = [], []\n",
    "\n",
    "for max_depth in max_depths:\n",
    "    learner1 = ml.dtree.treeClassify(\n",
    "        XtS, Yt, minParent=2, minLeaf=1, maxDepth=max_depth\n",
    "    )\n",
    "    learner2 = ml.dtree.treeClassify(\n",
    "        XtS, Yt, minParent=2, minLeaf=4, maxDepth=max_depth\n",
    "    )\n",
    "    l1.append(learner1.sz)\n",
    "    l2.append(learner2.sz)\n",
    "\n",
    "pt.plot(max_depths, l1, \"r\", label=\"Min leaf = 1\")\n",
    "pt.plot(max_depths, l2, label=\"Min leaf = 4\")\n",
    "pt.xlabel(\"Max depth\")\n",
    "pt.ylabel(\"Number of nodes\")\n",
    "pt.legend()\n",
    "pt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 5.3 Recommend a choice for minParent and minLeaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_parents = range(1, 20, 4)\n",
    "min_leaves = range(1, 10, 2)\n",
    "tr_auc = np.zeros((len(min_parents), len(min_leaves)))\n",
    "va_auc = np.zeros((len(min_parents), len(min_leaves)))\n",
    "\n",
    "for i,  min_parent in enumerate(min_parents):\n",
    "    for j, min_leaf in enumerate(min_leaves):\n",
    "        learner = ml.dtree.treeClassify(\n",
    "            XtS, Yt,\n",
    "            minParent=min_parent, minLeaf=min_leaf, maxDepth=30\n",
    "        )\n",
    "        tr_auc[i][j] = learner.auc(XtS, Yt)\n",
    "        va_auc[i][j] = learner.auc(XvS, Yva)\n",
    "\n",
    "f, ax = pt.subplots(1, 1, figsize=(8, 5))\n",
    "cax = ax.matshow(tr_auc, interpolation='nearest')\n",
    "f.colorbar(cax)\n",
    "ax.set_xticklabels(['']+list(min_leaves))\n",
    "ax.set_yticklabels(['']+list(min_parents))\n",
    "pt.ylabel(\"Minimum parent\")\n",
    "pt.xlabel(\"Minimum leaf\")\n",
    "pt.title(\"Training data AUC\")\n",
    "pt.show()\n",
    "\n",
    "f, ax = pt.subplots(1, 1, figsize=(8, 5))\n",
    "cax = ax.matshow(va_auc, interpolation='nearest')\n",
    "f.colorbar(cax)\n",
    "ax.set_xticklabels(['']+list(min_leaves))\n",
    "ax.set_yticklabels(['']+list(min_parents))\n",
    "pt.ylabel(\"Minimum parent\")\n",
    "pt.xlabel(\"Minimum leaf\")\n",
    "pt.title(\"Validation data AUC\")\n",
    "pt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " I would recommend Minimum parent as 1 and Minimum leaf as 7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 6. Neural Networks\n",
    " ## 6.1 Vary the number of hidden layers and the nodes in each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers, nodes = range(1, 6), range(2, 8)\n",
    "tr_auc = np.zeros((len(layers), len(nodes)))\n",
    "va_auc = np.zeros((len(layers), len(nodes)))\n",
    "for i, layer in enumerate(layers):\n",
    "    for j, node in enumerate(nodes):\n",
    "        nn = ml.nnet.nnetClassify()\n",
    "        nn.init_weights([XtS.shape[1], 5, 2], 'random', XtS, Yt)\n",
    "        nn.train(XtS, Yt, stopTol=1e-8, stepsize=.25, stopIter=300)\n",
    "        tr_auc[i][j] = nn.auc(XtS, Yt)\n",
    "        va_auc[i][j] = nn.auc(XvS, Yva)\n",
    "\n",
    "\n",
    "f, ax = pt.subplots(1, 1, figsize=(8, 5))\n",
    "cax = ax.matshow(tr_auc, interpolation='nearest')\n",
    "f.colorbar(cax)\n",
    "ax.set_xticklabels(['']+list(nodes))\n",
    "ax.set_yticklabels(['']+list(layers))\n",
    "pt.ylabel(\"Layer\")\n",
    "pt.xlabel(\"Node\")\n",
    "pt.title(\"Training data AUC\")\n",
    "pt.show()\n",
    "\n",
    "f, ax = pt.subplots(1, 1, figsize=(8, 5))\n",
    "cax = ax.matshow(va_auc, interpolation='nearest')\n",
    "f.colorbar(cax)\n",
    "ax.set_xticklabels(['']+list(nodes))\n",
    "ax.set_yticklabels(['']+list(layers))\n",
    "pt.ylabel(\"Layer\")\n",
    "pt.xlabel(\"Node\")\n",
    "pt.title(\"Validation data AUC\")\n",
    "pt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 6.2 Implement a custom activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Gaussian Training AUC: 0.49458635778635773\n",
    "\n",
    " Gaussian Validation AUC: 0.496823807293988\n",
    "\n",
    "\n",
    "\n",
    " Logistic Training AUC: 0.649433462033462\n",
    "\n",
    " Logistic Validation AUC: 0.6483055317573599\n",
    "\n",
    "\n",
    "\n",
    " HTangent Training AUC: 0.6832237666237666\n",
    "\n",
    " HTangent Validation AUC: 0.6660746786034655\n",
    "\n",
    " For both Training and Validation AUC, htangent seems to be the best activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Statement of Collaboration\n",
    " I have not collaborated with anyone for this homework."
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
